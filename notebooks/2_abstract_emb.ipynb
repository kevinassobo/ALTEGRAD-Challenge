{"cells":[{"cell_type":"markdown","metadata":{},"source":["# DOC2VEC EMBEDDINGS OF THE ABSTRACTS "]},{"cell_type":"markdown","metadata":{},"source":["The goal of this notebook is to train a Doc2Vec model on the abstracts of the papers, in order to obtain a semantic representation of them. \n","\n","After that we build a similarity graph of the papers with these embeddings."]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":252,"status":"ok","timestamp":1642091122113,"user":{"displayName":"Kevin Assobo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg4j2OMB-eDn_GWP0QZa2didejzV3_YzhTe-dl0gA=s64","userId":"09166701917392035402"},"user_tz":-60},"id":"o7YzcFXn5jni"},"outputs":[],"source":["import os \n","import numpy as np\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from gensim.models import KeyedVectors\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","import paths # script with all data paths"]},{"cell_type":"markdown","metadata":{},"source":["##  Doc2Vec embeddings"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The model was already trained !\n"]}],"source":["if not os.path.isfile(paths.DOC2VEC_PATH):\n","    nltk.download('stopwords')\n","    nltk.download('punkt')\n","    stop_words = set(stopwords.words('english')) \n","\n","    # Load the abstracts\n","    abstracts = dict()\n","    with open(paths.ABSTRACTS_PATH, 'r') as f:\n","        for line in f:\n","            node, abstract = line.split('|--|')\n","            abstracts[int(node)] = abstract \n","\n","    # Process the abstracts \n","    processed_abstracts = dict()\n","    for id_ in abstracts:\n","        processed_abstract = [\n","            word for word in word_tokenize(abstracts[id_].lower())\n","            if word.isalpha() and word not in stop_words\n","        ]\n","        processed_abstracts[id_] = processed_abstract\n","\n","    # Doc2Vec takes as input a list of TaggedDocument\n","    tagged_data = [\n","        TaggedDocument(abstract, [id_]) \n","        for id_, abstract in processed_abstracts.items()\n","    ]\n","\n","    dim = 64 # Embedding size\n","    # Train the model\n","    doc2vec_model = Doc2Vec(\n","        tagged_data, vector_size=dim, window=5,\n","        dm=1, min_count=2, epochs=100, workers=10\n","    )\n","    # Save the model\n","    doc2vec_model.save(paths.DOC2VEC_PATH)\n","else:\n","    print(\"The model was already trained !\")\n","    "]}],"metadata":{"colab":{"collapsed_sections":[],"name":"abstracts_embeddings.ipynb","provenance":[]},"interpreter":{"hash":"c13f75b521158b36ba3ff4ac87dceddef9f3d5f34c1f5e4e80e856c73bb49853"},"kernelspec":{"display_name":"Python 3.8.8 64-bit ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
