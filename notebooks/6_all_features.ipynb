{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPUTE ALL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "import paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read node pairs\n",
    "# Train\n",
    "train_pairs = pd.read_csv(\n",
    "    paths.TRAIN_PAIRS_PATH, \n",
    "    names=['node_1', 'node_2']\n",
    ")\n",
    "train_target = pd.read_csv(\n",
    "    paths.TRAIN_TARGET_PATH,\n",
    "    names=['target']\n",
    ").to_numpy().ravel()\n",
    "\n",
    "# Test\n",
    "test_pairs = pd.read_csv(\n",
    "    paths.TEST_PAIRS_PATH,\n",
    "    names=['node_1', 'node_2']\n",
    ")\n",
    "test_target = pd.read_csv(\n",
    "    paths.TEST_TARGET_PATH,\n",
    "    names=['target']\n",
    ").to_numpy().ravel()\n",
    "\n",
    "# For the Kaggle challenge\n",
    "challenge_pairs = pd.read_csv(\n",
    "    paths.CHALLENGE_PAIRS_PATH,\n",
    "    names=['node_1', 'node_2']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the training graph\n",
    "train_graph = nx.read_edgelist(\n",
    "    paths.TRAIN_EDGELIST_PATH, delimiter=',', nodetype=int\n",
    ")\n",
    "\n",
    "# Load the test graph\n",
    "test_graph = nx.read_edgelist(\n",
    "    paths.TEST_EDGELIST_PATH, delimiter=',', nodetype=int\n",
    ")\n",
    "\n",
    "# Load the full graph\n",
    "full_graph = nx.read_edgelist(\n",
    "    paths.FULL_GRAPH_EDGELIST_PATH, delimiter=',', nodetype=int\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the abstract of each paper\n",
    "abstracts = dict()\n",
    "with open(paths.ABSTRACTS_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        node, abstract = line.split('|--|')\n",
    "        abstracts[int(node)] = abstract\n",
    "\n",
    "# Read the authors to each paper\n",
    "authors = dict()\n",
    "with open(paths.PAPER_2_AUTHORS_ID_PATH, 'r') as f:\n",
    "    for line in f:\n",
    "        node, node_authors = line.rstrip('\\n').split('|--|')\n",
    "        authors[int(node)] = node_authors.split(',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the Doc2vec model\n",
    "doc2vec_model = Doc2Vec.load(paths.DOC2VEC_PATH)\n",
    "\n",
    "# Read nodes embeddings\n",
    "node2vec_train = KeyedVectors.load_word2vec_format(paths.NODE2VEC_TRAIN_PATH)\n",
    "node2vec_test = KeyedVectors.load_word2vec_format(paths.NODE2VEC_TEST_PATH)\n",
    "node2vec_full_graph = KeyedVectors.load_word2vec_format(paths.NODE2VEC_FULL_GRAPH_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Semantic, Attributes, Graph (paper citation network) based Features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Semantic Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Cosine similarity of abstract embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_doc2vec(paper_1, paper_2):\n",
    "    \"\"\"\n",
    "    Computes the cosine similarity between the abstract embeddings of two papers/nodes.\n",
    "    \"\"\"\n",
    "    # The model was trained on all the papers \n",
    "    return doc2vec_model.docvecs.similarity(paper_1, paper_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Attribute Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of common authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_authors(paper_1, paper_2):\n",
    "    \"\"\"\n",
    "    Computes the number of common author between two papers/nodes.\n",
    "    \"\"\"\n",
    "    # The model was trained on all the papers \n",
    "    return len(set(authors[paper_1]) & set(authors[paper_2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Graph based features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Degree approches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_diff_degree(node_1, node_2, G):\n",
    "    \"\"\"\n",
    "    Computes the difference in degree of two nodes in a graph.\n",
    "    \"\"\"\n",
    "    return abs(G.degree(node_1) - G.degree(node_2))\n",
    "\n",
    "\n",
    "def sum_degree(node_1, node_2, G):\n",
    "    \"\"\"\n",
    "    Computes the difference in degree of two nodes in a graph.\n",
    "    \"\"\"\n",
    "    return G.degree(node_1) + G.degree(node_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local based similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_coefficient(node_1, node_2, G):\n",
    "    \"\"\"\n",
    "    Computes the jaccard coefficient of two nodes in a graph.\n",
    "    \"\"\"\n",
    "    _, _, coeff = list(nx.jaccard_coefficient(G, [(node_1, node_2)]))[0]\n",
    "    return coeff\n",
    "\n",
    "\n",
    "def adamic_adar_index(node_1, node_2, G):\n",
    "    \"\"\"\n",
    "    Computes the adamic adar index of two nodes in a graph.\n",
    "    \"\"\"\n",
    "    _, _, index = list(nx.adamic_adar_index(G, [(node_1, node_2)]))[0]\n",
    "    return index\n",
    "\n",
    "\n",
    "def pref_attachment(node_1, node_2, G):\n",
    "    \"\"\" \n",
    "    Computes the preferential attachment of two nodes in a graph.\n",
    "    \"\"\"\n",
    "    _, _, p = list(nx.preferential_attachment(G, [(node_1, node_2)]))[0]\n",
    "    return p\n",
    "\n",
    "\n",
    "def salton_index(node_1, node_2, G):\n",
    "    \"\"\" \n",
    "    Computes the salton index of two nodes in a graph\n",
    "    \"\"\"\n",
    "    sqrt_prod = np.sqrt(G.degree(node_1) * G.degree(node_1))\n",
    "    if sqrt_prod == 0:\n",
    "        return 0\n",
    "    return len(list(nx.common_neighbors(G, node_1, node_2))) / sqrt_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Global based similarity measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shortest_path_length(node_1, node_2, G):\n",
    "    \"\"\"\n",
    "    Computes the shortest path length between two nodes in a graph.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        length = nx.shortest_path_length(G, node_1, node_2)\n",
    "    except nx.NetworkXNoPath:\n",
    "        length = -1\n",
    "    return length\n",
    "\n",
    "\n",
    "def diff_pagerank(node_1, node_2, pageranks):\n",
    "    \"\"\" \n",
    "    Computes the absolute difference in the pageranks of two nodes in a graph\n",
    "    \"\"\"\n",
    "    return np.abs(pageranks[node_1] - pageranks[node_2])\n",
    "\n",
    "\n",
    "def diff_eigvec_centrality(node_1, node_2, eigvec_centrality):\n",
    "    \"\"\" \n",
    "    Computes the absolute difference in the eigenvector centrality of two nodes in a graph\n",
    "    \"\"\"\n",
    "    return np.abs(eigvec_centrality[node_1] - eigvec_centrality[node_2])\n",
    "\n",
    "\n",
    "def diff_bet_centrality(node_1, node_2, bet_centrality):\n",
    "    \"\"\" \n",
    "    Computes the absolute difference in the betweeness centrality of two nodes in a graph\n",
    "    \"\"\"\n",
    "    return np.abs(bet_centrality[node_1] - bet_centrality[node_2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Computing features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(df, G, name):\n",
    "    print(f\"Computing features for the {name} set\")\n",
    "    print('-'*50)\n",
    "\n",
    "    t0 = time.time()\n",
    "    df[\"n_common_authors\"] = df.apply(\n",
    "        lambda row: common_authors(row['node_1'], row['node_2']),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"n_common_authors - done [{round(time.time() - t0,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"abstract_similarity\"] = df.apply(\n",
    "        lambda row: cosine_similarity_doc2vec(row['node_1'], row['node_2']),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"abstract_similarity - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"abs_diff_degree\"] = df.apply(\n",
    "        lambda row: abs_diff_degree(row['node_1'], row['node_2'], G),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"abs_diff_degree - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"sum_degree\"] = df.apply(\n",
    "        lambda row: sum_degree(row['node_1'], row['node_2'], G),\n",
    "        axis=1\n",
    "    ) \n",
    "    print(f\"sum_degree - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"jaccard_coeff\"] = df.apply(\n",
    "        lambda row: jaccard_coefficient(row['node_1'], row['node_2'], G),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"jaccard_coeff - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"adamic_adar_index\"] = df.apply(\n",
    "        lambda row: adamic_adar_index(row['node_1'], row['node_2'], G),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"adamic_adar_index - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"pref_attachment\"] = df.apply(\n",
    "        lambda row: pref_attachment(row['node_1'], row['node_2'], G),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"pref_attachment - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"salton_index\"] = df.apply(\n",
    "        lambda row: salton_index(row['node_1'], row['node_2'], G),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"salton_index - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    df[\"shortest_path_length\"] = df.apply(\n",
    "        lambda row: shortest_path_length(row['node_1'], row['node_2'], G),\n",
    "        axis=1\n",
    "    )\n",
    "    print(f\"shortest_path_length - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    eigvec_centrality = nx.eigenvector_centrality(G)\n",
    "    df[\"diff_eigvec_centrality\"] = df.apply(\n",
    "        lambda row: diff_eigvec_centrality(\n",
    "            row['node_1'], row['node_2'], eigvec_centrality\n",
    "        ), axis=1\n",
    "    )\n",
    "    print(f\"diff_eigvec_centrality - done [{round(time.time() - t1,1)}s]\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    pageranks = nx.pagerank(G)\n",
    "    df[\"diff_pagerank\"] = df.apply(\n",
    "        lambda row: diff_pagerank(\n",
    "            row['node_1'], row['node_2'], pageranks\n",
    "        ), axis=1\n",
    "    )\n",
    "    t2 = time.time()\n",
    "    print(f\"diff_pagerank - done [{round(t2 - t1,1)}s]\")\n",
    "    print(f\"Total time : [{round(t2 - t0,1)}s]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save the current computed features in a `pd.HDFStore`. We do this because computing all features is time expensive so we save these one first and then we can read them before computing other features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(paths.STORAGE_STAGE1_PATH):\n",
    "\n",
    "    # Create storage\n",
    "    hdf = pd.HDFStore(paths.STORAGE_STAGE1_PATH)\n",
    "\n",
    "    X_train = train_pairs.copy()\n",
    "    X_test = test_pairs.copy()\n",
    "    X_challenge = challenge_pairs.copy()\n",
    "    \n",
    "    # Train features\n",
    "    compute_features(X_train, train_graph, 'train')\n",
    "    hdf.put('X_train', X_train, format='table', data_columns=True)\n",
    "    # Test features\n",
    "    compute_features(X_test, test_graph, 'test')\n",
    "    hdf.put('X_test', X_test, format='table', data_columns=True)\n",
    "    # Challenge features\n",
    "    compute_features(X_challenge, full_graph, 'challenge')\n",
    "    hdf.put('X_challenge', X_challenge, format='table', data_columns=True)\n",
    "    # Close storage\n",
    "    hdf.close()\n",
    "else:\n",
    "    # Read the table if the storage exists\n",
    "    X_train = pd.read_hdf(paths.STORAGE_STAGE1_PATH, key='X_train', mode='r')\n",
    "    X_test = pd.read_hdf(paths.STORAGE_STAGE1_PATH, key='X_test', mode='r')\n",
    "    X_challenge = pd.read_hdf(paths.STORAGE_STAGE1_PATH, key='X_challenge', mode='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Features from the authors networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c13f75b521158b36ba3ff4ac87dceddef9f3d5f34c1f5e4e80e856c73bb49853"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
